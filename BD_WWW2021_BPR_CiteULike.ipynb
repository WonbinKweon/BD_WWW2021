{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "from torch.backends import cudnn\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import bottleneck as bn\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5219\n",
      "25187\n",
      "130799\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "inter = 5\n",
    "train = torch.load('data/cul/train_' + str(inter) + '.pt')\n",
    "val = torch.load('data/cul/val_' + str(inter) + '.pt')\n",
    "test = torch.load('data/cul/test_' + str(inter) + '.pt')\n",
    "\n",
    "train_matrix = torch.load('data/cul/train_matrix_' + str(inter) + '.pt')\n",
    "train_nei = np.load('data/cul/train_nei_' + str(inter) + '.npy').item()\n",
    "\n",
    "train_matrix_input = train_matrix.clone().type(torch.FloatTensor)\n",
    "for idx, (u,i) in enumerate(val):\n",
    "    train_matrix_input[u][val[idx][1]] = 0\n",
    "    train_matrix_input[u][test[idx][1]] = 0\n",
    "\n",
    "num_users = train_matrix.size()[0]\n",
    "num_items = train_matrix.size()[1]\n",
    "\n",
    "print(num_users)\n",
    "print(num_items)\n",
    "print(train.size()[0]+val.size()[0]*2)\n",
    "\n",
    "# for neg_sample\n",
    "matrix = train_matrix.numpy()\n",
    "neg_max = num_items - min(np.sum(matrix, axis = 1))\n",
    "neg_count = neg_max - np.sum(matrix, axis = 1)\n",
    "\n",
    "i, j = np.where(matrix == 0)\n",
    "user = 0\n",
    "count = 0\n",
    "negs = []\n",
    "for index, idx in enumerate(i):\n",
    "    if user < idx:\n",
    "        user = idx\n",
    "        neg = j[count:index].tolist()\n",
    "        neg += [-1]*(int(neg_max)-len(neg))\n",
    "        negs.append(neg)\n",
    "        count = index \n",
    "neg = j[count:].tolist()\n",
    "neg += [-1]*(int(neg_max)-len(neg))        \n",
    "negs.append(neg)\n",
    "negs_np = np.array(negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "class traindset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.shape(self.data)[0]\n",
    "\n",
    "val_dset = traindset(val)\n",
    "test_dset = traindset(test)\n",
    "train_dset = traindset(train)\n",
    "num_users = train_matrix.size()[0]\n",
    "num_items = train_matrix.size()[1]\n",
    "\n",
    "# negative sampling\n",
    "def get_train_instances(pairs, num_neg):  \n",
    "    users = pairs[:,0] \n",
    "    pos = pairs[:,1]\n",
    "    \n",
    "    uss, iss, jss = [],[],[]\n",
    "    for i in range(len(users)):\n",
    "        neg_idx = np.random.randint(0, neg_count[users[i]], size=(num_neg, ))\n",
    "        negs = negs_np[users[i], neg_idx]\n",
    "        for j in range(len(negs)):\n",
    "            uss.append(users[i])\n",
    "            iss.append(pos[i])\n",
    "            jss.append(negs[j])\n",
    "            \n",
    "    return torch.LongTensor(uss), torch.LongTensor(iss), torch.LongTensor(jss)\n",
    "\n",
    "def get_bd_instances(pairs, num_neg):\n",
    "    users = pairs[:,0]\n",
    "    pos = pairs[:,1]\n",
    "    \n",
    "    weight_T = weights_T[users]\n",
    "    weight_S = weights_S[users]\n",
    "\n",
    "    idx_T = torch.multinomial(weight_T, num_neg, replacement=True)\n",
    "    idx_S = torch.multinomial(weight_S, num_neg, replacement=True)\n",
    "\n",
    "    uss = torch.reshape(torch.reshape(users, (-1, 1)).expand(len(users), num_neg), (-1, ))\n",
    "    iss = torch.reshape(torch.reshape(pos, (-1, 1)).expand(len(users), num_neg), (-1, ))\n",
    "    jss_T = torch.reshape(idx_T, (-1,))\n",
    "    jss_S = torch.reshape(idx_S, (-1,))\n",
    "\n",
    "    return uss, iss, jss_T, jss_S\n",
    "\n",
    "def HR(k, eval_sort):\n",
    "    eval_sort = eval_sort.cpu().numpy()\n",
    "    _, idy = np.where(eval_sort == 0)\n",
    "    \n",
    "    return len(np.where(idy < k)[0]) / len(eval_sort)\n",
    "\n",
    "def NDCG(k, eval_sort):\n",
    "    eval_sort = eval_sort.cpu().numpy()\n",
    "    _, idy = np.where(eval_sort == 0)\n",
    "    rank = idy[np.where(idy < k)[0]]\n",
    "    \n",
    "    return np.sum(1 / np.log2(rank+2)) / len(eval_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base model - BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR(nn.Module):\n",
    "    def __init__(self, emb_dim, num_neg, matrix):\n",
    "        super(BPR, self).__init__()\n",
    "        self.num_users = matrix.size()[0]\n",
    "        self.num_items = matrix.size()[1]\n",
    "        self.u_emb = nn.Embedding(self.num_users, emb_dim) #max_norm = 1\n",
    "        self.i_emb = nn.Embedding(self.num_items, emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "    \n",
    "    def forward(self, u, i, j):\n",
    "        # embeddings\n",
    "        user_emb = self.u_emb(u)\n",
    "        pos_emb = self.i_emb(i)\n",
    "        neg_emb = self.i_emb(j)\n",
    "        \n",
    "        pos_score = torch.sum(torch.mul(user_emb, pos_emb), dim=1)\n",
    "        neg_score = torch.sum(torch.mul(user_emb, neg_emb), dim=1)\n",
    "\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    def evaluation(self, pairs):\n",
    "    # do not use this function\n",
    "        # indices\n",
    "        users = pairs[:, 0]\n",
    "        pos_items = pairs[:, 1].cpu().numpy().reshape((len(users), 1))\n",
    "        neg_items = np.zeros((len(users), 999))\n",
    "        for i in range(len(users)):\n",
    "            neg_idx = np.random.randint(0, neg_count[users[i]], size=(1, 999))\n",
    "            neg_items[i] = np.array(negs_np[users[i], neg_idx])\n",
    "        eval_items = torch.LongTensor(np.concatenate((pos_items, neg_items), axis=1)).cuda(gpu)\n",
    "        \n",
    "        # embeddings\n",
    "        user_emb = self.u_emb(users)\n",
    "        eval_emb = self.i_emb(eval_items)\n",
    "        \n",
    "        # distance\n",
    "        user_emb = torch.unsqueeze(user_emb, -1)\n",
    "        eval_dist = torch.sum(torch.mul(user_emb.transpose(1, 2).expand(len(users), 1000, self.emb_dim), eval_emb), dim = 2) # (N*100)\n",
    "        eval_sort = torch.argsort(eval_dist, dim=1, descending=True)\n",
    "        \n",
    "        return HR(5, eval_sort), HR(10, eval_sort), HR(20, eval_sort), NDCG(5, eval_sort), NDCG(10, eval_sort), NDCG(20, eval_sort)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "T_size = 50\n",
    "S_size = 5\n",
    "T = BPR(T_size, 1, train_matrix_input)\n",
    "S = BPR(S_size, 1, train_matrix_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 439.152, time = 3.9740\n",
      "epoch = 100, loss = 16.056, time = 3.9485\n",
      "epoch = 200, loss = 15.917, time = 4.0147\n",
      "epoch = 300, loss = 15.873, time = 3.8288\n",
      "epoch = 400, loss = 15.860, time = 4.0124\n",
      "epoch = 500, loss = 15.795, time = 3.7586\n",
      "epoch = 600, loss = 15.860, time = 3.7550\n",
      "epoch = 700, loss = 15.707, time = 3.7986\n",
      "epoch = 800, loss = 15.877, time = 3.7538\n",
      "epoch = 900, loss = 15.833, time = 3.7650\n"
     ]
    }
   ],
   "source": [
    "# Teacher\n",
    "use_cuda = torch.cuda.is_available()\n",
    "bs = 128\n",
    "num_neg = 1\n",
    "lr = 0.001\n",
    "wd = 0.001\n",
    "epochs = 1000\n",
    "verbose = 100\n",
    "\n",
    "# data\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dset, batch_size = bs, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dset, batch_size = 1024, shuffle = False)\n",
    "\n",
    "optimizer = optim.Adam(T.parameters(), lr = lr, weight_decay=wd)\n",
    "if use_cuda:\n",
    "    T = T.cuda(gpu)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    T.train()\n",
    "    loss_train = np.zeros(3)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, pairs in enumerate(train_loader):\n",
    "        u, i, j = get_train_instances(pairs, num_neg)\n",
    "        if use_cuda:\n",
    "            u, i, j = u.cuda(gpu), i.cuda(gpu), j.cuda(gpu) \n",
    "\n",
    "        ### train\n",
    "        optimizer.zero_grad()\n",
    "        pos, neg = T(u, i, j)\n",
    "\n",
    "        loss_dist = - torch.log(torch.sigmoid(pos - neg)).sum()\n",
    "        loss = loss_dist \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train[0] += loss.cpu().tolist() \n",
    "    loss_train /= len(train_loader)\n",
    "\n",
    "    if epoch % verbose == 0:\n",
    "        print('epoch = {}, loss = {:.3f}, time = {:.4f}'.format(epoch, loss_train[0], time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 174.849, time = 3.2463\n",
      "epoch = 100, loss = 20.485, time = 3.2589\n",
      "epoch = 200, loss = 13.370, time = 3.4075\n",
      "epoch = 300, loss = 11.361, time = 3.5590\n",
      "epoch = 400, loss = 10.384, time = 3.2499\n",
      "epoch = 500, loss = 9.741, time = 3.2246\n",
      "epoch = 600, loss = 9.612, time = 3.2309\n",
      "epoch = 700, loss = 9.192, time = 3.3215\n",
      "epoch = 800, loss = 9.020, time = 3.3509\n",
      "epoch = 900, loss = 8.655, time = 3.2460\n"
     ]
    }
   ],
   "source": [
    "# Student\n",
    "use_cuda = torch.cuda.is_available()\n",
    "bs = 128\n",
    "num_neg = 1\n",
    "lr = 0.001\n",
    "wd = 0\n",
    "epochs = 1000\n",
    "verbose = 100\n",
    "\n",
    "# data\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dset, batch_size = bs, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dset, batch_size = 1024, shuffle = False)\n",
    "\n",
    "optimizer = optim.Adam(S.parameters(), lr = lr, weight_decay = wd)\n",
    "if use_cuda:\n",
    "    S = S.cuda(gpu)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    S.train()\n",
    "    loss_train = np.zeros(3)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, pairs in enumerate(train_loader):\n",
    "        u, i, j = get_train_instances(pairs, num_neg)\n",
    "        if use_cuda:\n",
    "            u, i, j = u.cuda(gpu), i.cuda(gpu), j.cuda(gpu) \n",
    "\n",
    "        ### train\n",
    "        optimizer.zero_grad()\n",
    "        pos, neg = S(u, i, j)\n",
    "\n",
    "        loss_dist = - torch.log(torch.sigmoid(pos - neg)).sum()\n",
    "        loss = loss_dist \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train[0] += loss.cpu().tolist() \n",
    "    loss_train /= len(train_loader)\n",
    "\n",
    "    if epoch % verbose == 0:\n",
    "        print('epoch = {}, loss = {:.3f}, time = {:.4f}'.format(epoch, loss_train[0], time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520300\n",
      "152030\n",
      "epoch = 0, loss = 56.920+192.609= 0.000, time = 21.2252\n",
      "0.1152, 0.1774, 0.2684, 0.0286, 0.0386, 0.0513\n",
      "0.0680, 0.1106, 0.1757, 0.0159, 0.0228, 0.0319\n",
      "epoch = 50, loss = 48.574+89.676= 0.000, time = 21.4700\n",
      "0.1422, 0.2159, 0.3114, 0.0409, 0.0528, 0.0661\n",
      "0.0879, 0.1307, 0.1933, 0.0224, 0.0293, 0.0381\n",
      "epoch = 100, loss = 49.312+78.750= 0.000, time = 22.0093\n",
      "0.1429, 0.2219, 0.3232, 0.0412, 0.0540, 0.0681\n",
      "0.0849, 0.1257, 0.1997, 0.0223, 0.0289, 0.0392\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "lamb_T = 0.5\n",
    "lamb_S = 0.5\n",
    "lamb_CF = 1\n",
    "\n",
    "temp_T = 5\n",
    "temp_S = 5\n",
    "\n",
    "neg_KD = 1\n",
    "eps = 1e-4\n",
    "eps_t = 1e-2\n",
    "wd = 0\n",
    "\n",
    "update = 10\n",
    "epochs = 100\n",
    "verbose = 50\n",
    "\n",
    "batch_size = 128\n",
    "lr_T = 0.001\n",
    "lr_S = 0.001\n",
    "\n",
    "# model\n",
    "print(sum(p.numel() for p in T.parameters()))\n",
    "print(sum(p.numel() for p in S.parameters()))\n",
    "\n",
    "# loss\n",
    "optimizer_T = optim.Adam(T.parameters(), lr = lr_T, weight_decay=wd)\n",
    "optimizer_S = optim.Adam(S.parameters(), lr = lr_S)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum') # for L_BD\n",
    "\n",
    "if use_cuda:\n",
    "    T = T.cuda(gpu)\n",
    "    S = S.cuda(gpu)\n",
    "\n",
    "for epoch in range(0, epochs+1):\n",
    "    loss_train = np.zeros(3)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # update rank matrix\n",
    "    if (epoch % update == 0):\n",
    "        with torch.no_grad():\n",
    "            T.eval()\n",
    "            S.eval()\n",
    "            rank_matrix_T = torch.zeros_like(train_matrix_input).type(torch.LongTensor)\n",
    "            rank_matrix_S = torch.zeros_like(train_matrix_input).type(torch.LongTensor)\n",
    "            mask_matrix = (1-train_matrix_input).cuda(gpu).float()     \n",
    "\n",
    "            for u in range(num_users):\n",
    "                uss = (torch.ones(num_items) * u).type(torch.LongTensor).cuda(gpu)\n",
    "                iss = torch.LongTensor(np.arange(num_items)).cuda(gpu)\n",
    "                jss = torch.LongTensor(np.arange(num_items)).cuda(gpu)\n",
    "                _, row1 = T(uss, iss, jss)\n",
    "                _, row2 = S(uss, iss, jss)\n",
    "                \n",
    "                row_mask1 = row1.view(num_items) * mask_matrix[u]            \n",
    "                rank_list1 = torch.argsort(row_mask1)\n",
    "                rank_matrix_T[u] = rank_list1\n",
    "                row_mask2 = row2.view(num_items) * mask_matrix[u]              \n",
    "                rank_list2 = torch.argsort(row_mask2)\n",
    "                rank_matrix_S[u] = rank_list2 \n",
    "\n",
    "            ranklist_T = torch.zeros_like(rank_matrix_T)\n",
    "            for i in range(len(ranklist_T)):\n",
    "                row = rank_matrix_T[i]\n",
    "                ranklist_T[i][row] = torch.LongTensor(np.arange(len(row))) + 1\n",
    "            ranklist_S = torch.zeros_like(rank_matrix_S)\n",
    "            for i in range(len(ranklist_S)):\n",
    "                row = rank_matrix_S[i]\n",
    "                ranklist_S[i][row] = torch.LongTensor(np.arange(len(row))) + 1\n",
    "            \n",
    "            rank_dif_T = ranklist_T - ranklist_S\n",
    "            rank_dif_S = ranklist_S - ranklist_T\n",
    "\n",
    "            weights_T = torch.exp(rank_dif_T.type(torch.FloatTensor) * eps).cuda(gpu)\n",
    "            weights_S = torch.tanh(torch.max(rank_dif_S.type(torch.FloatTensor) * eps_t, torch.zeros_like(rank_dif_S).type(torch.FloatTensor))).cuda(gpu)\n",
    "\n",
    "    T.train()\n",
    "    S.train()\n",
    "    # training\n",
    "    for batch_idx, pairs in enumerate(train_loader):\n",
    "        u, i, j_T, j_S = get_bd_instances(pairs, neg_KD)\n",
    "        u, i, j_T, j_S = u.cuda(gpu), i.cuda(gpu), j_T.cuda(gpu), j_S.cuda(gpu)\n",
    "\n",
    "        ### train\n",
    "        optimizer_T.zero_grad()\n",
    "        optimizer_S.zero_grad()\n",
    "\n",
    "        # For BD T->S\n",
    "        _, neg2T = T(u, i, j_S)\n",
    "        pos2, neg2 = S(u, i, j_S)\n",
    "        # For BD S->T\n",
    "        _, neg1S = S(u, i, j_T)\n",
    "        pos1, neg1 = T(u, i, j_T) \n",
    "\n",
    "        # loss for T\n",
    "        loss_T_CF = - torch.log(torch.sigmoid(pos1 - neg1)).sum()\n",
    "        pseudo_label = torch.sigmoid(neg1S / temp_S).detach() \n",
    "        loss_T_WS = criterion(neg1, pseudo_label)\n",
    "        loss_T = loss_T_CF * lamb_CF + loss_T_WS * lamb_T\n",
    "        loss_T.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # loss for S\n",
    "        loss_S_CF = - torch.log(torch.sigmoid(pos2 - neg2)).sum()\n",
    "        pseudo_label = torch.sigmoid(neg2T / temp_T).detach() \n",
    "        loss_S_WS = criterion(neg2, pseudo_label)\n",
    "        loss_S = loss_S_CF * lamb_CF + loss_S_WS * lamb_S\n",
    "        loss_S.backward()\n",
    "        optimizer_S.step()\n",
    "        \n",
    "        #loss_train[0] += loss.cpu().tolist() \n",
    "        loss_train[1] += loss_T.cpu().tolist() \n",
    "        loss_train[2] += loss_S.cpu().tolist()\n",
    "    loss_train /= len(train_loader)\n",
    "\n",
    "    if epoch % verbose == 0:\n",
    "        print('epoch = {}, loss = {:.3f}+{:.3f}= {:.3f}, time = {:.4f}'.format(epoch, loss_train[1], loss_train[2], loss_train[0], time.time()-t0))\n",
    "        ## full val\n",
    "        rank_T = []\n",
    "        for row in test:\n",
    "            row = row.numpy()\n",
    "            rank_T.append(num_items - np.where(rank_matrix_T[row[0]] == row[1])[0][0])\n",
    "        rank_T = np.array(rank_T)\n",
    "        ndcg = 1 / np.log2(rank_T + 2)\n",
    "\n",
    "        print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format((rank_T < 50).mean(), (rank_T < 100).mean(), (rank_T < 200).mean(), (ndcg * (rank_T < 50)).mean(), (ndcg * (rank_T < 100)).mean(), (ndcg * (rank_T < 200)).mean()))\n",
    "    \n",
    "        rank_S = []\n",
    "        for row in test:\n",
    "            row = row.numpy()\n",
    "            rank_S.append(num_items - np.where(rank_matrix_S[row[0]] == row[1])[0][0])\n",
    "        rank_S = np.array(rank_S)\n",
    "        ndcg = 1 / np.log2(rank_S + 2)\n",
    "\n",
    "        print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format((rank_S < 50).mean(), (rank_S < 100).mean(), (rank_S < 200).mean(), (ndcg * (rank_S < 50)).mean(), (ndcg * (rank_S < 100)).mean(), (ndcg * (rank_S < 200)).mean()))                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python37164bitbaseconda7d83067cfd6040cb9b83bec122768017"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
